import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
import re
import pandas as pd

data = pd.read_csv('https://raw.githubusercontent.com/AdityaDutt-PythonDeveloper/Natural-Language-Processing/refs/heads/main/tripadvisor_hotel_reviews.csv')

data.head()
data.info()

data['Review'][0]
data['review_lowercase'] = data['Review'].str.lower()

nltk.download('stopwords')
en_stopwords = stopwords.words('english')
en_stopwords.remove('not')
data['review_no_stopwords'] = data['review_lowercase'].apply(lambda x : ' '.join([word for word in x.split() if word not in en_stopwords]))
data['review_no_stopwords'][0]

import re

data['review_no_stopwords_no_punct'] = data.apply(
    lambda x: re.sub(r"[^\w\s]", "", x['review_no_stopwords_no_punct']),
    axis=1
)

data['tokenized'] = data.apply(lambda x : word_tokenize(x['review_no_stopwords_no_punct']), axis = 1)
data['tokenized'][0]

ps = PorterStemmer()
data['stemmed'] = data['tokenized'].apply(lambda tokens : [ps.stem(token) for token in tokens])

lemmatizer = WordNetLemmatizer()
data['lemmatized'] = data['tokenized'].apply(lambda tokens : [lemmatizer.lemmatize(token) for token in tokens])

tokens_clean = sum(data['lemmatized'], [])
unigrams = pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts()

